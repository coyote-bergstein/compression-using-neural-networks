{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import glob\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_ds(url, download_path, target_path):\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to download file. Status code: {response.status_code}\")\n",
    "        return\n",
    "    \n",
    "    with open(download_path, 'wb') as file:\n",
    "        shutil.copyfileobj(response.raw, file)\n",
    "    print(\"Download completed successfully.\")\n",
    "        \n",
    "    with zipfile.ZipFile(download_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(target_path)\n",
    "    print(\"Extraction completed successfully.\")\n",
    "        \n",
    "    os.remove(download_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free Music Archive (FMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_url = 'https://os.unil.cloud.switch.ch/fma/fma_small.zip'\n",
    "\n",
    "download_path = 'train_ds.zip'\n",
    "\n",
    "train_ds_path = 'train_ds'\n",
    "\n",
    "if not os.path.exists(train_ds_path):\n",
    "    download_ds(train_ds_url, download_path, train_ds_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_BATCHES = 4\n",
    "TRAIN_DS_SIZE = NUM_BATCHES * BATCH_SIZE\n",
    "AUDIO_SAMPLE_RATE = 8000\n",
    "TRACK_DURATION = 1 # seconds\n",
    "MAX_AUDIO_LENGTH = AUDIO_SAMPLE_RATE * TRACK_DURATION\n",
    "EPOCHS = 2\n",
    "\n",
    "LATENT_DIM = 128\n",
    "LEARING_RATE = 0.00005\n",
    "N_FFT = 1024  # Define the FFT window size to reduce frequency bins\n",
    "HOP_LENGTH = 256  # Define the hop length (adjust as needed)\n",
    "OPTIMIZER = 'adam'\n",
    "LOSS = 'mse'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spectrogram(y, sr, n_fft=N_FFT, hop_length=HOP_LENGTH):\n",
    "    D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length)\n",
    "    S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "    return S_db\n",
    "\n",
    "def normalize_spectrogram(S_db):\n",
    "    S_db_norm = (S_db - S_db.min()) / (S_db.max() - S_db.min())\n",
    "    return S_db_norm\n",
    "\n",
    "def pad_or_truncate_spectrogram(S_db_norm, max_length=MAX_AUDIO_LENGTH):\n",
    "    current_length = S_db_norm.shape[1]\n",
    "    if current_length < max_length:\n",
    "        padding = max_length - current_length\n",
    "        S_db_norm = np.pad(S_db_norm, ((0, 0), (0, padding)), mode='constant')\n",
    "    else:\n",
    "        S_db_norm = S_db_norm[:, :max_length]\n",
    "    return S_db_norm\n",
    "\n",
    "def prepare_input_for_autoencoder(S_db_norm):\n",
    "    S_db_norm = np.expand_dims(S_db_norm, axis=-1)\n",
    "    return S_db_norm\n",
    "\n",
    "def load_audio_as_spectrogram(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=AUDIO_SAMPLE_RATE, mono=True, duration=10)  # Load only first 10 seconds\n",
    "    S_db = compute_spectrogram(y, sr, hop_length=HOP_LENGTH)  # Adjust hop length to reduce time frames\n",
    "    S_db_norm = normalize_spectrogram(S_db)\n",
    "    S_db_norm = pad_or_truncate_spectrogram(S_db_norm)\n",
    "    input_data = prepare_input_for_autoencoder(S_db_norm)\n",
    "    return input_data, y  # Return both the spectrogram and the original audio\n",
    "\n",
    "def revert_spectrogram(S_db_norm, sr=AUDIO_SAMPLE_RATE, hop_length=HOP_LENGTH):\n",
    "    S_db = S_db_norm * 80 - 80  # Reverting normalization\n",
    "    S = librosa.db_to_amplitude(S_db)\n",
    "    y = librosa.istft(S, hop_length=hop_length)\n",
    "    return y\n",
    "\n",
    "def get_all_mp3_paths(root_dir):\n",
    "    mp3_paths = glob.glob(os.path.join(root_dir, '**/*.mp3'), recursive=True)\n",
    "    return mp3_paths\n",
    "\n",
    "# Wrapper function to use with TensorFlow\n",
    "def load_audio_as_spectrogram_wrapper(file_path):\n",
    "    spectrogram, _ = load_audio_as_spectrogram(file_path.numpy().decode('utf-8'))\n",
    "    return tf.convert_to_tensor(spectrogram, dtype=tf.float32)\n",
    "\n",
    "def prepare_dataset(mp3_paths):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(mp3_paths)\n",
    "    dataset = dataset.map(lambda x: tf.py_function(load_audio_as_spectrogram_wrapper, [x], tf.float32), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (32, 513, 8000, 1)\n",
      "Data shape: (32, 513, 8000, 1)\n",
      "Data shape: (32, 513, 8000, 1)\n",
      "Data shape: (32, 513, 8000, 1)\n"
     ]
    }
   ],
   "source": [
    "mp3_paths = get_all_mp3_paths(train_ds_path)\n",
    "train_ds = prepare_dataset(mp3_paths[:TRAIN_DS_SIZE])\n",
    "input_shape = (N_FFT // 2 + 1, MAX_AUDIO_LENGTH, 1)\n",
    "\n",
    "for data in train_ds:\n",
    "    print(f\"Data shape: {data.shape}\")\n",
    "    assert data.shape == (BATCH_SIZE, *input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check (TODO)\n",
    "Lets define function for reverting spectrogram to audio and check if reverting train_ds produces input autio (module IPython.display for listening audio in notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original and reconstructed audio files for validation samples saved.\n"
     ]
    }
   ],
   "source": [
    "# Take 5 samples for validation/comparison\n",
    "validation_samples = mp3_paths[:5]\n",
    "\n",
    "# Process the validation samples\n",
    "validation_spectrograms = []\n",
    "validation_original_audios = []\n",
    "for file_path in validation_samples:\n",
    "    S_db_norm, original_audio = load_audio_as_spectrogram(file_path)\n",
    "    validation_spectrograms.append(S_db_norm)\n",
    "    validation_original_audios.append(original_audio)\n",
    "\n",
    "# Reconstruct the audio from the validation spectrograms\n",
    "validation_reconstructed_audios = [revert_spectrogram(S_db_norm.squeeze()) for S_db_norm in validation_spectrograms]\n",
    "\n",
    "# Save and compare the original and reconstructed audio\n",
    "for i, (original, reconstructed) in enumerate(zip(validation_original_audios, validation_reconstructed_audios)):\n",
    "    sf.write(f'original_audio_{i}.wav', original, AUDIO_SAMPLE_RATE)\n",
    "    sf.write(f'reconstructed_audio_{i}.wav', reconstructed, AUDIO_SAMPLE_RATE)\n",
    "\n",
    "print(\"Original and reconstructed audio files for validation samples saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revert_spectrogram(S_db_norm, sr=AUDIO_SAMPLE_RATE, hop_length=HOP_LENGTH):\n",
    "    # Revert normalization\n",
    "    S_db = S_db_norm * (S_db_norm.max() - S_db_norm.min()) + S_db_norm.min()\n",
    "    # Convert dB-scaled spectrogram back to amplitude\n",
    "    S = librosa.db_to_amplitude(S_db * 80 - 80)  # Scale back to original range\n",
    "    # Inverse STFT\n",
    "    y = librosa.istft(S, hop_length=hop_length)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_autoencoder(input_shape):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    # Encoder\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    encoded = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "    # Decoder\n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(encoded)\n",
    "    x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
    "    decoded = tf.keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "    \n",
    "    autoencoder = tf.keras.Model(inputs, decoded)\n",
    "    autoencoder.compile(optimizer=OPTIMIZER, loss=LOSS)\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = build_autoencoder(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "autoencoder.fit(train_ds.map(lambda x: (x, x)), epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_loss(y_true, y_pred):\n",
    "    spectrogram_true = tf.signal.stft(y_true, frame_length=256, frame_step=64)\n",
    "    spectrogram_pred = tf.signal.stft(y_pred, frame_length=256, frame_step=64)\n",
    "    magnitude_true = tf.abs(spectrogram_true)\n",
    "    magnitude_pred = tf.abs(spectrogram_pred)\n",
    "    return tf.reduce_mean(tf.abs(magnitude_true - magnitude_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/audio_autoencoder_20240522_034441\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/audio_autoencoder_20240522_034441\\assets\n"
     ]
    }
   ],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_save_path = f'models/audio_autoencoder_{current_time}'\n",
    "autoencoder.save(model_save_path, save_format='tf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GlobalDsEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
